# chatbot_backend.py (or in your app.py)
import joblib
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from rag import rag_lookup
from flow_manage import update_state, construct_prompt_with_state, generate_response
from slot_extract import handle_rental_post

# Initialize models and classifier once (do this outside the function!)
model_name = "liswei/Taiwan-ELM-270M-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).eval()
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
embedder = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
clf = joblib.load('intent_classifier.pkl')

def classify(text):
    emb = embedder.encode([text])
    return clf.predict(emb)[0]

def fallback_or_escalate(user_input):
    escalate_keywords = ["å‘Š", "æ³•é™¢", "ä»²ä»‹è³ å„Ÿ", "å¾‹å¸«", "æ³•å¾‹è²¬ä»»"]
    if any(kw in user_input for kw in escalate_keywords):
        return "ğŸš¨ é€™å¯èƒ½æ¶‰åŠæ³•å¾‹ç¨‹åºï¼Œå»ºè­°è«®è©¢å¾‹å¸«æˆ–æ³•å¾‹å°ˆæ¥­äººå£«ã€‚"
    return None

def chatbot_response(user_input, chat_state):
    if not user_input.strip():
        return ""

    classification = classify(user_input)
    update_state(user_input, classification, chat_state)  # Pass chat_state here if needed

    warning = fallback_or_escalate(user_input)
    if warning:
        return warning

    if classification == "greeting":
        return "å“ˆå›‰ï½è«‹å•æœ‰ä»€éº¼ç§Ÿå±‹ç›¸é—œçš„å•é¡Œæˆ‘å¯ä»¥å¹«å¿™ï¼Ÿ"
    
    elif classification == "other":
        return "è«‹å•ç§Ÿå±‹ç›¸é—œå•é¡Œæˆ–åœ¨èªªçš„æ¸…æ¥šä¸€äº›"

    elif classification == "rental_post":
        result = handle_rental_post(user_input)
        chat_state.setdefault("fb_post_info", {}).update(result["parsed_info"])
        chat_state["last_topic_type"] = "rental_post"
        missing = [k for k, v in result["parsed_info"].items() if v == "ç„¡"]
        if missing and chat_state.get("update_chance", True):
            chat_state["update_chance"] = False
            return f"ç›®å‰\n{result['advice']} å»ºè­°è£œå……"
        else:
            reply = (
                f"ä»¥ä¸‹æ˜¯ç›®å‰æ“·å–çš„å…§å®¹ï¼š\n{result['parsed_info']}\n\n"
                f"æ ¹æ“šä¸Šè¿°å…§å®¹ï¼Œé€™æ˜¯æˆ‘çš„å»ºè­°ï¼š\n{result['consequence']}"
            )
            return reply

    else:
        rag_answer = rag_lookup(user_input)
        if rag_answer:
            return f"åŠ©ç†ï¼ˆRAGï¼‰ï¼š{rag_answer}"
        else:
            prompt = construct_prompt_with_state(user_input)
            prompt += "\nè«‹æä¾›èˆ‡ç§Ÿå±‹ç›¸é—œçš„å»ºè­°æˆ–é¢¨éšªæé†’ï¼š"
            answer = generate_response(model, tokenizer, prompt)
            return f"åŠ©ç†ï¼ˆLLMï¼‰ï¼š{answer}"
