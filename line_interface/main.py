import joblib
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import re
from typing import Dict, List
from utils.rag import rag_lookup
from utils.flow_manage import update_state, construct_prompt_with_state, generate_response
from utils.slot_extract import handle_rental_post

def fallback_or_escalate(user_input):
    escalate_keywords = ["å‘Š", "æ³•é™¢", "ä»²ä»‹è³ å„Ÿ", "å¾‹å¸«", "æ³•å¾‹è²¬ä»»"]#å¯åœ¨æ›´å‹•
    if any(kw in user_input for kw in escalate_keywords):
        return "ğŸš¨ é€™å¯èƒ½æ¶‰åŠæ³•å¾‹ç¨‹åºï¼Œå»ºè­°è«®è©¢å¾‹å¸«æˆ–æ³•å¾‹å°ˆæ¥­äººå£«ã€‚"
    return None


# åˆå§‹åŒ–å°è©±ç‹€æ…‹
chat_state = {
    "fb_post_info": {},          # ä½¿ç”¨è€…è²¼æ–‡è³‡è¨Šï¼ˆä¾‹å¦‚ï¼šåœ°é»ã€ç§Ÿé‡‘ï¼‰
    "user_questions": [],        # ä½¿ç”¨è€…å•éçš„å•é¡Œ
    "issues_reported": [],       # ä½¿ç”¨è€…æéçš„ç§Ÿå±‹ç³¾ç´›
    "region_context": None,      # ç§Ÿå±‹å€åŸŸ
    "last_topic_type": None,      # ä¸Šæ¬¡çš„ä»»å‹™é¡å‹
    "update_chance":True
}
model_name="liswei/Taiwan-ELM-270M-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True).eval()#.cuda()
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
embedder = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
clf = joblib.load('intent_classifier.pkl')
def classify(text):
    emb = embedder.encode([text])
    return clf.predict(emb)[0]

def chat_loop_with_state(model=model, tokenizer=tokenizer, chat_state=chat_state):
    print("é–‹å§‹èŠå¤©ï¼Œè¼¸å…¥ç©ºå­—ä¸²å¯é€€å‡º")

    while True:
        user_input = input("ä½ ï¼š").strip()
        if user_input == "":#ç©ºå­—ä¸²çµæŸ
            break

        #åˆ†é¡å°è©±æ„åœ–+æ›´æ–°ç‹€æ…‹
        classification = classify(user_input)
        update_state(user_input, classification)
        print(f"åˆ†é¡ï¼š{classification}")

        # æ•æ„Ÿå­—æª¢æ¸¬
        warning = fallback_or_escalate(user_input)
        if warning:
            print("åŠ©ç†ï¼š", warning)
            #continue

        if classification == "greeting":#ä¸€èˆ¬å•å€™
            print("åŠ©ç†ï¼šå“ˆå›‰ï½è«‹å•æœ‰ä»€éº¼ç§Ÿå±‹ç›¸é—œçš„å•é¡Œæˆ‘å¯ä»¥å¹«å¿™ï¼Ÿ")
            continue
        
        elif classification=="other":
            print("è«‹å•ç§Ÿå±‹ç›¸é—œå•é¡Œæˆ–åœ¨èªªçš„æ¸…æ¥šä¸€äº›")
            continue

        elif classification == "rental_post":#ç§Ÿå±‹è²¼æ–‡
            result = handle_rental_post(user_input)

            chat_state.setdefault("fb_post_info", {}).update(result["parsed_info"])
            chat_state["last_topic_type"] = "rental_post"
            missing = [k for k, v in result["parsed_info"].items() if v == "ç„¡"]
            if missing and chat_state["update_chance"]:
                #chat_state["pending_slots"] = missing
                print("åŠ©ç†ï¼šç›®å‰\n")
                print(result["advice"]+"å»ºè­°è£œå……\n")
                chat_state["update_chance"]=False
                continue
            else:
                # è‹¥ç„¡ç¼ºå¤±ï¼Œç›´æ¥ç”¢ç”Ÿå»ºè­°
                print("åŠ©ç†ï¼šä»¥ä¸‹æ˜¯ç›®å‰æ“·å–çš„å…§å®¹ï¼š")
                print(result["parsed_info"])
                print("\n æ ¹æ“šä¸Šè¿°å…§å®¹ï¼Œé€™æ˜¯æˆ‘çš„å»ºè­°ï¼š")
                print(result["consequence"])
                continue


        else:
            rag_answer = rag_lookup(user_input)
            if rag_answer:
              print(f"åŠ©ç†ï¼ˆRAGï¼‰ï¼š{rag_answer}")
            else:
              prompt = construct_prompt_with_state(user_input)
              prompt += "\nè«‹æä¾›èˆ‡ç§Ÿå±‹ç›¸é—œçš„å»ºè­°æˆ–é¢¨éšªæé†’ï¼š"
              answer = generate_response(model,tokenizer,prompt)
              print(f"åŠ©ç†ï¼ˆLLMï¼‰ï¼š{answer}")

chat_loop_with_state()